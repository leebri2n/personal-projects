{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "id": "qU58nGMHeRpH",
        "cellView": "code"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "!python -m pip install TextBlob\n",
        "!python -m pip install newspaper3k\n",
        "!python -m pip install preprocessor\n",
        "!python -m pip install Twython\n",
        "!python -m pip install tweepy\n",
        "!python -m pip install os\n",
        "!python -m pip install nltk\n",
        "!python -m pip install tensorflow\n",
        "!python -m pip install contractions\n",
        "!python -m pip install autocorrect\n",
        "!python -m pip install betterspy\n",
        "!python -m pip install tensorflow\n",
        "!python -m pip install cvxopt import matrix, solvers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qQqIiDA5eRpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eae6dbe-907b-4bdb-e89e-cd377c114838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Load packages\n",
        "import os\n",
        "import time\n",
        "\n",
        "from textblob import TextBlob\n",
        "from newspaper import Article\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "import requests\n",
        "import preprocessor as pre\n",
        "import csv\n",
        "import json\n",
        "import string\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from contractions import contractions_dict\n",
        "from autocorrect import Speller\n",
        "import seaborn as sns\n",
        "import pylab as pl\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import tensorflow\n",
        "\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "from twython import Twython"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Setup"
      ],
      "metadata": {
        "id": "STD6K4v-0PHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling directories\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df_movie = pd.read_csv('/content/drive/My Drive/Spring 2022/CS4824/finalproj/movies/movie.csv')\n",
        "\n",
        "#Append the directory to your python path\n",
        "prefix = '/content/drive/My Drive/'\n",
        "# modify customized_path\n",
        "customized_path = 'Spring 2022/CS4824/finalproj/'\n",
        "data_folder = 'movies/'\n",
        "sys_path = prefix + customized_path + data_folder\n",
        "sys.path.append(sys_path)\n",
        "print(sys.path)\n",
        "\n",
        "data_filename = os.path.join(sys_path, 'movie.csv')\n",
        "print('Path to training data: {}'.format(data_filename))\n",
        "#fn_test = os.path.join(sys_path, './')\n",
        "#print('Path to testing data: {}'.format(fn_test))"
      ],
      "metadata": {
        "id": "QBeZjm1ZAW-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d9edfbe-2566-4a22-fb96-f5f949690984"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython', '/content/drive/My Drive/Spring 2022/CS4824/finalproj/movies/']\n",
            "Path to training data: /content/drive/My Drive/Spring 2022/CS4824/finalproj/movies/movie.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading in and splitting training and test data\n",
        "df_orig = pd.read_csv(data_filename)\n",
        "df_orig = df_orig.drop(labels=range(20001, 40000))\n",
        "\n",
        "print(df_orig)"
      ],
      "metadata": {
        "id": "Ct9kufztEt1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42156abe-6fa2-43bf-ddf0-f9bd0c4c6bde"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  label\n",
            "0      I grew up (b. 1965) watching and loving the Th...      0\n",
            "1      When I put this movie in my DVD player, and sa...      0\n",
            "2      Why do people who do not know what a particula...      0\n",
            "3      Even though I have great interest in Biblical ...      0\n",
            "4      Im a die hard Dads Army fan and nothing will e...      1\n",
            "...                                                  ...    ...\n",
            "19996  \"White Noise\" had potential to be one of the m...      0\n",
            "19997  The Five Deadly Venoms is a great kung-fu acti...      1\n",
            "19998  Ali G Indahouse has got to be one of the funni...      1\n",
            "19999  I found myself at sixes and sevens while watch...      1\n",
            "20000  Christopher Lambert is annoying and disappoint...      0\n",
            "\n",
            "[20001 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "wz0EqfbE0Qwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase(text):\n",
        "  \"\"\"\n",
        "    Converts text to be completely lowercase.\n",
        "\n",
        "    Argument: String of text.\n",
        "    Returns: Completely lowercase version of input text.\n",
        "  \"\"\"\n",
        "  return text.lower()\n",
        "  \n",
        "def remove_stopwords(text):\n",
        "  \"\"\"\n",
        "    Removes all stopwords from the text.\n",
        "    Note: Refers to nltk's default English stopword list.\n",
        "\n",
        "    Argument: String of text.\n",
        "    Returns: Input text, now free of stopwords.\n",
        "  \"\"\"\n",
        "  #Treat text as sentence\n",
        "  stopword_list = stopwords.words('english')\n",
        "  return ' '.join([word for word in nltk.word_tokenize(text) if not word in stopword_list])\n",
        "\n",
        "def lemmatize(text_tagged):\n",
        "  \"\"\"\n",
        "    Normalizes all words in the text\n",
        "    Note: Utilizes nltk's WordNet's lemmatizer to derive lemmatizations.\n",
        "\n",
        "    Argument: A list of tuples: [(<word>, <part of speech retag>),...]\n",
        "    Returns: Lemmatized form of text\n",
        "  \"\"\"\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized = \"\"\n",
        "  for word, pos in text_tagged:\n",
        "    if not pos:\n",
        "      lemmatized = lemmatized + \" \" + word\n",
        "    else:\n",
        "      lemmatized = lemmatized + \" \" + lemmatizer.lemmatize(word, pos=pos)\n",
        "\n",
        "  return lemmatized\n",
        "\n",
        "def retag(text_tagged):\n",
        "  \"\"\"\n",
        "    Rewrites the part of speech tags for each word into a form\n",
        "    interpretable by WordNet.\n",
        "\n",
        "    Argument: A list of tuples: [(<word>, <part of speech tag>),...]\n",
        "    Returns: Lemmatized form of text\n",
        "  \"\"\"\n",
        "  pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
        "  retagged = []\n",
        "  for word, tag in text_tagged:\n",
        "      retagged.append(tuple([word, pos_dict.get(tag[0])]))\n",
        "\n",
        "  return retagged\n",
        "\n",
        "def spellcheck(text):\n",
        "  corrections = [Speller.spell(word) for word in nltk.word_tokenize(text)]\n",
        "  return ' '.join(corrections)\n",
        "\n",
        "\n",
        "#All functions combined\n",
        "def preprocessing3(text):\n",
        "  \"\"\"\n",
        "    Aggregation of preprocessing functions into one.\n",
        "    Note: Additionally uses re's sub() function for convenient cleaning.\n",
        "\n",
        "    Argument: Text, as a string.\n",
        "    Returns: Preprocessed text.\n",
        "  \"\"\"\n",
        "  lower = lowercase(text)\n",
        "  cleaned = re.sub('[^A-Za-z]+', ' ', lower)\n",
        "  stopwordfree = remove_stopwords(cleaned)\n",
        "  tokenized = word_tokenize(stopwordfree)\n",
        "  pos = nltk.pos_tag(tokenized)\n",
        "  posretagged = retag(pos)\n",
        "  lemmatized = lemmatize(posretagged)\n",
        "  tokenized2 = word_tokenize(lemmatized)\n",
        "  \n",
        "  #return tokenized2 #final, option1\n",
        "  return lemmatized #final, option2"
      ],
      "metadata": {
        "id": "sOErue-ClvIE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extext = df_orig.iloc[50]['text']\n",
        "\n",
        "print(extext)\n",
        "print(preprocessing3(extext))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3hwNc9khm35",
        "outputId": "b119a623-16ce-4e1d-94b7-07b8865cfd54"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When this first came out, my dad brought it home- we were amazed by it- It was so different from anything we had seen before. I was looking for a specific movie last night, and I found \"The Mind's Eye\" again. The box is falling apart, and I am surprised that the tape still works! Although it is not 'Finding Nemo' quality graphics, it is still very good. They should sell this again- it is a landmark for computer animation imagery. Highly recommended!<br /><br />This is what it is:<br /><br />\"The Mind's Eye\" is a spectacular odyssey through time. Your journey begins at the dawn of creation and moves through the rise of man and technology. Travel in the world of abstraction and on into the future with breathtaking computer animation imagery.<br /><br />\"The Mind's Eye\" joins the imaginations of over 300 of the world's most talented computer animation artists with a powerful, original music soundtrack. This unique collaboration takes you on an incredible voyage into \"The Mind's Eye.\"\n",
            " first come dad bring home amaze different anything see look specific movie last night find mind eye box fall apart surprise tape still work although find nemo quality graphic still good sell landmark computer animation imagery highly recommend br br br br mind eye spectacular odyssey time journey begin dawn creation move rise man technology travel world abstraction future breathtaking computer animation imagery br br mind eye join imagination world talented computer animation artist powerful original music soundtrack unique collaboration take incredible voyage mind eye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_labels = df_orig['label']\n",
        "df_text = df_orig['text'].apply(preprocessing3, 'expand')\n",
        "\n",
        "print(df_text.head())"
      ],
      "metadata": {
        "id": "t8wmWL7xOY35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bcc7578-4f20-4e94-ce1e-1f1183e772a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     grow b watch loving thunderbird mat school wa...\n",
            "1     put movie dvd player sit coke chip expectatio...\n",
            "2     people know particular time past like feel ne...\n",
            "3     even though great interest biblical movie bor...\n",
            "4     im die hard dad army fan nothing ever change ...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Test Split"
      ],
      "metadata": {
        "id": "xiRjIONL0aHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_proc = {'text': df_text, 'label': df_labels}\n",
        "df_proc = pd.DataFrame(data=data_proc)\n",
        "print(df_proc)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df_proc, test_size=0.2)\n",
        "\n",
        "train_data = train_df['text']\n",
        "train_labs = train_df['label']\n",
        "test_data = test_df['text']\n",
        "test_labs = test_df['label']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7b6P7cwi3xQ",
        "outputId": "29dd1abb-1933-4e0d-9ac3-0b89c9f8442e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  label\n",
            "0       grow b watch loving thunderbird mat school wa...      0\n",
            "1       put movie dvd player sit coke chip expectatio...      0\n",
            "2       people know particular time past like feel ne...      0\n",
            "3       even though great interest biblical movie bor...      0\n",
            "4       im die hard dad army fan nothing ever change ...      1\n",
            "...                                                  ...    ...\n",
            "19996   white noise potential one talk movie since ex...      0\n",
            "19997   five deadly venom great kung fu action movie ...      1\n",
            "19998   ali g indahouse get one funny film see long t...      1\n",
            "19999   find six sevens watch one altman touch zoom e...      1\n",
            "20000   christopher lambert annoy disappointing portr...      0\n",
            "\n",
            "[20001 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Vectorization"
      ],
      "metadata": {
        "id": "eIYnUvar0BNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=100)\n",
        "vectorizer.fit(df_proc['text'])\n",
        "\n",
        "train_data_tfidf = vectorizer.transform(train_data)\n",
        "test_data_tfidf = vectorizer.transform(test_data)"
      ],
      "metadata": {
        "id": "zA7YHaHmi5rL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vectorizer.vocabulary_))\n",
        "print(type(train_data_tfidf))\n",
        "print(train_data_tfidf.shape)\n",
        "print(train_data_tfidf[0])\n",
        "print(np.max(train_data_tfidf))\n",
        "\n",
        "Xtrain = train_data_tfidf.toarray()\n",
        "Xtest = test_data_tfidf.toarray()\n",
        "ytrain = train_labs.to_numpy()\n",
        "ytest = train_labs.to_numpy()\n",
        "\n",
        "print(Xtrain.shape)\n",
        "print(Xtrain)\n",
        "print(ytrain.shape)\n",
        "print(ytrain.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-RieYqRaCVw",
        "outputId": "7bf52c5f-d090-4ffb-df6c-dc7e4191cadd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "(16000, 100)\n",
            "  (0, 96)\t0.20191785489033795\n",
            "  (0, 95)\t0.2982031584782163\n",
            "  (0, 92)\t0.18847432307568485\n",
            "  (0, 74)\t0.28736025576726243\n",
            "  (0, 55)\t0.14718939284678614\n",
            "  (0, 49)\t0.13604868275388562\n",
            "  (0, 46)\t0.26742195117252415\n",
            "  (0, 45)\t0.15722639045956285\n",
            "  (0, 42)\t0.21790997999208273\n",
            "  (0, 39)\t0.16318711066953273\n",
            "  (0, 36)\t0.22022317642833636\n",
            "  (0, 31)\t0.19274762015740782\n",
            "  (0, 24)\t0.1449538840701364\n",
            "  (0, 18)\t0.22115012331190595\n",
            "  (0, 10)\t0.5844066055047122\n",
            "  (0, 7)\t0.21114497090463594\n",
            "1.0\n",
            "(16000, 100)\n",
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.13081209 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.25488511 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.20897456 0.         0.         ... 0.         0.23248711 0.        ]\n",
            " [0.         0.20909471 0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.32442026 0.        ]]\n",
            "(16000,)\n",
            "[0 0 1 ... 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection"
      ],
      "metadata": {
        "id": "0hrAFAHEJcyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vf27VIweo4p2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fcu8xuJwv2l1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.count_nonzero(Xtrain[1,:])/len(Xtrain[1,:]))\n",
        "print(train_data_tfidf[1,:].getnnz()/80506)\n",
        "\n",
        "print(train_data_tfidf[1035,:])\n",
        "print(train_data_tfidf[:,1])\n",
        "#print(np.dot(train_data_tfidf[1,:], train_data_tfidf[5,:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXuSvzCylunI",
        "outputId": "b045ad0f-38d4-4a23-e165-14172816f2b8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.23\n",
            "0.0002856929918266962\n",
            "  (0, 1)\t0.15502356529668732\n",
            "  (0, 3)\t0.1414281612393872\n",
            "  (0, 11)\t0.17835585619916994\n",
            "  (0, 12)\t0.12648820566978783\n",
            "  (0, 16)\t0.18401945522688842\n",
            "  (0, 18)\t0.13918826313821095\n",
            "  (0, 24)\t0.3649263958203485\n",
            "  (0, 28)\t0.11151593117851612\n",
            "  (0, 36)\t0.27720971592322613\n",
            "  (0, 37)\t0.18070163825583493\n",
            "  (0, 38)\t0.156541208194308\n",
            "  (0, 41)\t0.18418668858127107\n",
            "  (0, 44)\t0.1472762999944608\n",
            "  (0, 45)\t0.19791142667998296\n",
            "  (0, 48)\t0.18834732353338032\n",
            "  (0, 51)\t0.15525909511405214\n",
            "  (0, 52)\t0.18430642791196367\n",
            "  (0, 55)\t0.09263859154090294\n",
            "  (0, 61)\t0.18354420619710285\n",
            "  (0, 67)\t0.13787248421020032\n",
            "  (0, 68)\t0.14034943343506187\n",
            "  (0, 69)\t0.10214632448070711\n",
            "  (0, 70)\t0.1539894486853819\n",
            "  (0, 73)\t0.1678722887570896\n",
            "  (0, 75)\t0.1821869621228259\n",
            "  (0, 80)\t0.14409692043747144\n",
            "  (0, 81)\t0.1296617086259546\n",
            "  (0, 86)\t0.15751349926443134\n",
            "  (0, 92)\t0.1186226499998623\n",
            "  (0, 93)\t0.18652864293477617\n",
            "  (0, 96)\t0.25416757719046135\n",
            "  (0, 99)\t0.18608248422986853\n",
            "  (4, 0)\t0.13729169428986218\n",
            "  (5, 0)\t0.11345986026985609\n",
            "  (8, 0)\t0.04859458388580016\n",
            "  (9, 0)\t0.1483674411492152\n",
            "  (16, 0)\t0.177385977361532\n",
            "  (17, 0)\t0.13176156028879557\n",
            "  (18, 0)\t0.1430091104053981\n",
            "  (20, 0)\t0.2342314909941652\n",
            "  (26, 0)\t0.11382163619163051\n",
            "  (28, 0)\t0.06225611518640686\n",
            "  (36, 0)\t0.13892564807450408\n",
            "  (44, 0)\t0.10328114389892165\n",
            "  (45, 0)\t0.10679820228630917\n",
            "  (46, 0)\t0.12002982492770936\n",
            "  (61, 0)\t0.07610217812506201\n",
            "  (64, 0)\t0.19109575369689308\n",
            "  (72, 0)\t0.26481499530246555\n",
            "  (74, 0)\t0.12364887187228654\n",
            "  (75, 0)\t0.06161975225428284\n",
            "  (86, 0)\t0.31338519958234395\n",
            "  (87, 0)\t0.07059792421009821\n",
            "  (95, 0)\t0.11238169512066791\n",
            "  (103, 0)\t0.14439053442790578\n",
            "  (105, 0)\t0.1024795634480434\n",
            "  (107, 0)\t0.24901233006744827\n",
            "  :\t:\n",
            "  (15853, 0)\t0.27220974851137064\n",
            "  (15871, 0)\t0.2638441844164624\n",
            "  (15876, 0)\t0.10268904836127839\n",
            "  (15882, 0)\t0.3260652136600053\n",
            "  (15888, 0)\t0.15055819386375935\n",
            "  (15893, 0)\t0.15718630698009692\n",
            "  (15919, 0)\t0.15924884470965495\n",
            "  (15920, 0)\t0.22840450008474478\n",
            "  (15923, 0)\t0.19311627125501452\n",
            "  (15924, 0)\t0.13085448394369414\n",
            "  (15930, 0)\t0.14690164974665626\n",
            "  (15937, 0)\t0.2670171669849667\n",
            "  (15943, 0)\t0.06486157511144681\n",
            "  (15947, 0)\t0.061721110719362315\n",
            "  (15955, 0)\t0.17659274049644577\n",
            "  (15959, 0)\t0.10190144989006492\n",
            "  (15968, 0)\t0.15391174205522706\n",
            "  (15972, 0)\t0.21322624913757177\n",
            "  (15973, 0)\t0.1125432987951652\n",
            "  (15976, 0)\t0.07372488537697469\n",
            "  (15985, 0)\t0.15141661612825563\n",
            "  (15987, 0)\t0.22083481317087375\n",
            "  (15989, 0)\t0.10241562017102168\n",
            "  (15992, 0)\t0.12233117608739283\n",
            "  (15998, 0)\t0.20909471123684079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math \n",
        "\n",
        "#print(train_data[0])\n",
        "\n",
        "def txf(sentence):\n",
        "  word_dict = {}\n",
        "  words = word_tokenize(sentence);\n",
        "  \n",
        "  for word in words:\n",
        "    word_dict[word] = word_dict.get(word, 0) + 1\n",
        "  return word_dict\n",
        "\n",
        "print(txf(extext))\n",
        "print(len(df_orig))\n",
        "\n",
        "class TFIDF:\n",
        "  def __init__(self):\n",
        "    self.word_index = {}\n",
        "    self.index_word = {}\n",
        "    self.idf_dict = {}\n",
        "  \n",
        "  def fit(self, data):\n",
        "    text_list = data\n",
        "    text_num = len(text_list)\n",
        "\n",
        "    global_tf = {}\n",
        "    for text in text_list:\n",
        "      wordset = set()\n",
        "      text_tf = txf(text)\n",
        "\n",
        "      for word in text_tf:\n",
        "        if word not in wordset:\n",
        "          global_tf[word] = global_tf.get(word,0) + 1\n",
        "          wordset.add(word)\n",
        "\n",
        "    for word, freq in global_tf.items():\n",
        "      idf = math.log((1+len(data)) / (1+freq))\n",
        "      self.idf_dict[word] = idf\n",
        "\n",
        "    text_words = list(global_tf.keys())\n",
        "    for i in range(len(text_words)):\n",
        "      word = text_words[i]\n",
        "      self.word_index[word] = i\n",
        "      self.index_word[i] = word\n",
        "\n",
        "    print(type(global_tf.items()))\n",
        "\n",
        "  def transform(self, data):\n",
        "    text_vects = []\n",
        "    text_list = data\n",
        "\n",
        "    for text in text_list:\n",
        "      text_vects.append(self.transformtext(data, text))\n",
        "\n",
        "    return np.matrix(text_vects)\n",
        "\n",
        "  def transformtext(self, data, text):\n",
        "    text_list = data\n",
        "    text_tokenized = word_tokenize(text)\n",
        "    \n",
        "    word_vector = np.zeros(len(self.word_index))\n",
        "    text_tfidf = self.text_tfidf(text)\n",
        "\n",
        "    for word in text_tokenized:\n",
        "      if word in self.word_index:\n",
        "        word_index = self.word_index[word]\n",
        "        word_vector[word_index] = text_tfidf[word]\n",
        "\n",
        "    return word_vector\n",
        "\n",
        "  def text_tfidf(self, text):\n",
        "    text_tfidf = {}\n",
        "    text_tf = txf(text)\n",
        "    num_words = sum(text_tf.values())\n",
        "    \n",
        "    average_freq = {k:(float(v)/num_words) for k, v in text_tf.items()}\n",
        "\n",
        "    for term, tf in average_freq.items():\n",
        "      text_tfidf[term] = tf*self.idf_dict.get(term, 0)\n",
        "\n",
        "    return text_tfidf"
      ],
      "metadata": {
        "id": "8a3GYcHP_P5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50cdd48-184f-460c-aa63-697f2fa366a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'When': 1, 'this': 2, 'first': 1, 'came': 1, 'out': 1, ',': 5, 'my': 1, 'dad': 1, 'brought': 1, 'it': 5, 'home-': 1, 'we': 2, 'were': 1, 'amazed': 1, 'by': 1, 'it-': 1, 'It': 1, 'was': 2, 'so': 1, 'different': 1, 'from': 1, 'anything': 1, 'had': 1, 'seen': 1, 'before': 1, '.': 8, 'I': 3, 'looking': 1, 'for': 2, 'a': 4, 'specific': 1, 'movie': 1, 'last': 1, 'night': 1, 'and': 5, 'found': 1, '``': 2, 'The': 5, 'Mind': 4, \"'s\": 5, 'Eye': 4, \"''\": 6, 'again': 1, 'box': 1, 'is': 7, 'falling': 1, 'apart': 1, 'am': 1, 'surprised': 1, 'that': 1, 'the': 7, 'tape': 1, 'still': 2, 'works': 1, '!': 2, 'Although': 1, 'not': 1, \"'Finding\": 1, 'Nemo': 1, \"'\": 1, 'quality': 1, 'graphics': 1, 'very': 1, 'good': 1, 'They': 1, 'should': 1, 'sell': 1, 'again-': 1, 'landmark': 1, 'computer': 3, 'animation': 3, 'imagery': 1, 'Highly': 1, 'recommended': 1, '<': 6, 'br': 6, '/': 6, '>': 6, 'This': 2, 'what': 1, ':': 1, 'spectacular': 1, 'odyssey': 1, 'through': 2, 'time': 1, 'Your': 1, 'journey': 1, 'begins': 1, 'at': 1, 'dawn': 1, 'of': 5, 'creation': 1, 'moves': 1, 'rise': 1, 'man': 1, 'technology': 1, 'Travel': 1, 'in': 1, 'world': 2, 'abstraction': 1, 'on': 2, 'into': 2, 'future': 1, 'with': 2, 'breathtaking': 1, 'imagery.': 1, 'joins': 1, 'imaginations': 1, 'over': 1, '300': 1, 'most': 1, 'talented': 1, 'artists': 1, 'powerful': 1, 'original': 1, 'music': 1, 'soundtrack': 1, 'unique': 1, 'collaboration': 1, 'takes': 1, 'you': 1, 'an': 1, 'incredible': 1, 'voyage': 1}\n",
            "20001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test = TFIDF()\n",
        "#test.fit(train_data)\n",
        "#test.transform(train_data)\n",
        "#print(test.index_word)"
      ],
      "metadata": {
        "id": "arlJyV2EQ0V0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Algorithms\n"
      ],
      "metadata": {
        "id": "MDL3aqURz-Ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "9CkRCTV2QCG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "D8MCTRzHz52c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(data, predictions):\n",
        "  data = data.to_numpy()\n",
        "  correct = 0\n",
        "\n",
        "  for i in range(len(predictions)):\n",
        "    if (float(data[i]) == 1.0 and predictions[i][0] == 1.0) or (float(data[i]) == 0.0 and predictions[i][0] == 0.0):\n",
        "      correct += 1\n",
        "\n",
        "  return float(correct/len(predictions))"
      ],
      "metadata": {
        "id": "YM_onX-7nPwo"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation of SVM class inspired by \n",
        "\n",
        "from cvxopt import matrix, solvers\n",
        "\n",
        "class SVM():\n",
        "  def __init__(self, C):\n",
        "    #C, the stretch parameter\n",
        "    self.C = C\n",
        "    self.kernel = self.kernel_lin\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    m, n = Xtrain.shape\n",
        "    y = y.reshape(-1, 1)\n",
        "    y = y.astype('float')\n",
        "\n",
        "    K = self.kernel(X, X)\n",
        "\n",
        "    self.labels = np.unique(y)\n",
        "    print(self.labels)\n",
        "    \n",
        "    recode = y == self.labels[0]\n",
        "    y[recode] = 1       \n",
        "    y[~recode] = -1\n",
        "    y = y * 1.\n",
        "\n",
        "    assert(X.shape[0] == y.shape[0])\n",
        "\n",
        "    P = matrix(np.matmul(y,y.T)*K)\n",
        "    q = matrix(np.ones((m, 1))*-1)\n",
        "    A = matrix(y.reshape(1, -1))\n",
        "    b = matrix(np.zeros(1))\n",
        "    G = matrix(np.vstack((np.eye(m) * -1, np.eye(m))))        \n",
        "    h = matrix(np.hstack((np.zeros(m), np.ones(m) * self.C)))\n",
        "    print(A.size)\n",
        "\n",
        "    sol = solvers.qp(P,q,G,h,A,b)\n",
        "\n",
        "    alpha = np.array(sol['x'])\n",
        "    ind = (alpha > 1e-4).flatten()\n",
        "\n",
        "    self.sv = X[ind]\n",
        "    self.sv_y = y[ind]\n",
        "    self.alpha = alpha[ind]\n",
        "\n",
        "    b = self.sv_y - np.sum(self.kernel(self.sv, self.sv) * self.alpha * self.sv_y, axis = 0)\n",
        "    self.b = np.sum(b)/b.size\n",
        "\n",
        "    self.w = np.sum(self.alpha * self.sv_y * self.sv, axis=0)\n",
        "\n",
        "  def kernel_lin(self, u, v):\n",
        "    return np.dot(u, v.T)\n",
        "\n",
        "  def predict(self, Xtest):\n",
        "        if self.w is None:\n",
        "            print(\"No model constructed.\")\n",
        "            return\n",
        "        else:\n",
        "            pred = np.dot(self.w, Xtest.T) + self.b\n",
        "        ypred = np.sign(pred)\n",
        "        ypred = ypred.reshape(-1, 1)\n",
        "        recode = ypred == 1.\n",
        "        ypred[recode] = self.labels[0]\n",
        "        ypred[~recode] = self.labels[1]\n",
        "\n",
        "        return ypred\n",
        "  "
      ],
      "metadata": {
        "id": "MNs6_gDmsWM5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = SVM(0.5)\n",
        "print(Xtrain.shape, ytrain.shape)\n",
        "print(type(Xtrain), type(ytrain))\n",
        "test.fit(Xtrain, ytrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPUkSp-F0MBc",
        "outputId": "3d7874f0-711b-4a75-b88e-69137a1c3ce2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16000, 100) (16000,)\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "[0. 1.]\n",
            "(1, 16000)\n",
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -8.2724e+03 -2.7384e+04  3e+05  8e+00  4e-14\n",
            " 1: -4.0221e+03 -2.3545e+04  5e+04  9e-01  4e-14\n",
            " 2: -3.4825e+03 -1.1878e+04  1e+04  9e-02  2e-14\n",
            " 3: -3.9835e+03 -5.4920e+03  2e+03  1e-02  2e-14\n",
            " 4: -4.4290e+03 -4.9621e+03  6e+02  4e-03  3e-14\n",
            " 5: -4.5536e+03 -4.8130e+03  3e+02  2e-03  3e-14\n",
            " 6: -4.6062e+03 -4.7497e+03  1e+02  8e-04  3e-14\n",
            " 7: -4.6358e+03 -4.7137e+03  8e+01  4e-04  3e-14\n",
            " 8: -4.6558e+03 -4.6896e+03  3e+01  1e-04  3e-14\n",
            " 9: -4.6650e+03 -4.6788e+03  1e+01  4e-05  3e-14\n",
            "10: -4.6693e+03 -4.6737e+03  4e+00  8e-06  3e-14\n",
            "11: -4.6706e+03 -4.6724e+03  2e+00  3e-06  3e-14\n",
            "12: -4.6712e+03 -4.6717e+03  5e-01  4e-07  3e-14\n",
            "13: -4.6714e+03 -4.6715e+03  2e-01  1e-07  3e-14\n",
            "14: -4.6714e+03 -4.6715e+03  3e-02  9e-09  3e-14\n",
            "15: -4.6715e+03 -4.6715e+03  2e-03  5e-10  3e-14\n",
            "Optimal solution found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test.w)\n",
        "print(len(test.w))\n",
        "print(type(test.w))\n",
        "\n",
        "model = test.w\n",
        "pred = test.predict(Xtest)\n",
        "\n",
        "print(accuracy(test_labs, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "XaZbJ1DhUzIR",
        "outputId": "878c998d-35ef-478e-ae5c-90a95d534c5b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.09860299  1.06714971  0.71838416 -1.7893525   0.48496924  0.55121612\n",
            " -0.38773026  6.62810632 -2.64352775 -0.13356428  0.13060877 -0.20675531\n",
            "  0.16910031 -0.51963857 -0.5359877   1.56531478 -0.7114173   1.3379691\n",
            " -0.05398679  2.3681271   0.31052066 -0.63670005  0.29249828 -0.59976748\n",
            " -0.61677613 -0.69598712 -0.54869709 -0.08198894  0.43702895  0.48519839\n",
            "  0.32175558  1.03828884 -0.73032076 -3.7374284   0.58963835  0.44194041\n",
            " -0.58440978  0.34077279 -1.5276078   0.03648101 -0.35642904 -0.01243995\n",
            "  1.3427708  -0.67418524 -2.52516561  0.67117835 -0.2757355  -0.98767435\n",
            "  2.53281026  0.21987184  0.50628894  0.40405681 -0.83125111  3.70903763\n",
            "  0.31693085 -0.18680347 -0.24899466  0.13630334 -1.58680841 -0.72839729\n",
            "  1.60292994  1.31070043  1.17429963 -0.9788435   0.07117167  0.27084579\n",
            " -0.88165817  0.96524836  0.44026199 -1.25726275  1.42904085 -0.14707607\n",
            " -0.61303662  0.51578482  0.02676161  0.68410148 -1.69459633 -0.61050815\n",
            " -0.51148297 -0.77478606  0.81417122 -0.39552029 -0.97654228 -0.03623697\n",
            "  1.87660553  0.00942777  0.03853359 -0.71309226  0.34686308  0.66196901\n",
            "  0.18202593 -0.60733602 -1.65523673  0.46477509 -0.60531494 -1.70209912\n",
            "  1.82787385  1.25566102 -1.2104406  -1.00963144]\n",
            "100\n",
            "<class 'numpy.ndarray'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-08bb025d4b27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-0addf7feeb3e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mPredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \"\"\"\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SVM' object has no attribute 'normalize'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Neural Network (DNN)"
      ],
      "metadata": {
        "id": "0pDPxiyH0jSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "n = 80000;\n",
        "num_classes=2\n",
        "def dnn(hidden_layers=0):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(n, input_shape=train_data_tfidf[1:]))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  for i in range(hidden_layers):\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=n))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "qIZ58io20k77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = dict()\n",
        "\n",
        "for i in range(2):\n",
        "  print(\"Training DNN:\")\n",
        "  model = dnn(i)\n",
        "  history[i] = model.fit(train_data_tfidf, train_labs, batch_size=5, epochs=3, validation_data=(test_data_tfidf, test_labs),shuffle=True)"
      ],
      "metadata": {
        "id": "F6_XuGUe-zgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "SVM = svm.SVC(C=1.0, kernel=\"linear\", degree=3, gamma='auto')\n",
        "SVM.fit(train_data_tfidf, train_labs)\n",
        "\n",
        "SVM_pred = SVM.predict(test_data_tfidf)\n",
        "\n",
        "print(\"SVM accuracy:\", accuracy_score(SVM_pred))"
      ],
      "metadata": {
        "id": "ra3pxvXSLzBO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Research Project - Sentiment Analysis NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}